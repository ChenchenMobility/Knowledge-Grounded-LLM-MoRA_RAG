{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TifsNNtHDcdU"
   },
   "source": [
    "# ğŸ¤– Agentic RAG Model\n",
    "- subset of data, no router version\n",
    "\n",
    "### Workflow\n",
    "\n",
    "- ğŸ“‚ **Load different ChromaDB databases**  \n",
    "- ğŸ§  **Run with ChatGPT OSS as the backbone model for the agent**  \n",
    "- ğŸ“œ **Apply router to decide which RAG agent to use**  \n",
    "- ğŸ“ **Save results**  \n",
    "  - Final generated output  \n",
    "  - Which search agent was used  \n",
    "  - Ground truth answer for comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4U2_DQXEEeRJ"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 66965,
     "status": "ok",
     "timestamp": 1761855459105,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "bSYhhZpnDEVA",
    "outputId": "b8257d94-2d5c-40d1-fac1-5e84342e7488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Downloading chromadb-1.3.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.10)\n",
      "Collecting pybase64>=1.4.1 (from chromadb)\n",
      "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.28.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.71.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Downloading chromadb-1.3.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m119.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=0caca3d4a6e10ab9387a8fa26e1632b80afc50c79911fa43fa3c370231135323\n",
      "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, durationpy, uvloop, urllib3, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, posthog, opentelemetry-semantic-conventions, onnxruntime, opentelemetry-sdk, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.5.0\n",
      "    Uninstalling urllib3-2.5.0:\n",
      "      Successfully uninstalled urllib3-2.5.0\n",
      "  Attempting uninstall: opentelemetry-proto\n",
      "    Found existing installation: opentelemetry-proto 1.37.0\n",
      "    Uninstalling opentelemetry-proto-1.37.0:\n",
      "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
      "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
      "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
      "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
      "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "    Found existing installation: opentelemetry-api 1.37.0\n",
      "    Uninstalling opentelemetry-api-1.37.0:\n",
      "      Successfully uninstalled opentelemetry-api-1.37.0\n",
      "  Attempting uninstall: opentelemetry-semantic-conventions\n",
      "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
      "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
      "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
      "  Attempting uninstall: opentelemetry-sdk\n",
      "    Found existing installation: opentelemetry-sdk 1.37.0\n",
      "    Uninstalling opentelemetry-sdk-1.37.0:\n",
      "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
      "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 chromadb-1.3.0 coloredlogs-15.0.1 durationpy-0.10 httptools-0.7.1 humanfriendly-10.0 kubernetes-34.1.0 mmh3-5.2.0 onnxruntime-1.23.2 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-grpc-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 urllib3-2.3.0 uvloop-0.22.1 watchfiles-1.1.1\n",
      "Collecting litellm\n",
      "  Downloading litellm-1.79.0-py3-none-any.whl.metadata (42 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.12/dist-packages (from litellm) (3.13.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from litellm) (8.3.0)\n",
      "Collecting fastuuid>=0.13.0 (from litellm)\n",
      "  Downloading fastuuid-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (8.7.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from litellm) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (4.25.1)\n",
      "Requirement already satisfied: openai>=1.99.5 in /usr/local/lib/python3.12/dist-packages (from litellm) (1.109.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (2.11.10)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (1.2.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from litellm) (0.22.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (1.22.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=6.8.0->litellm) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.28.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (0.11.1)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->litellm) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->litellm) (2.32.4)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers->litellm) (0.36.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (6.0.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.3.0)\n",
      "Downloading litellm-1.79.0-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastuuid-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fastuuid, litellm\n",
      "Successfully installed fastuuid-0.14.0 litellm-1.79.0\n",
      "Collecting qdrant-client\n",
      "  Downloading qdrant_client-1.15.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (1.76.0)\n",
      "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.0.2)\n",
      "Collecting portalocker<4.0,>=2.7.0 (from qdrant-client)\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (5.29.5)\n",
      "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.11.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.41.0->qdrant-client) (4.15.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.2)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
      "Downloading qdrant_client-1.15.1-py3-none-any.whl (337 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: portalocker, qdrant-client\n",
      "Successfully installed portalocker-3.2.0 qdrant-client-1.15.1\n",
      "Collecting ddgs\n",
      "  Downloading ddgs-9.6.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from ddgs) (8.3.0)\n",
      "Collecting primp>=0.15.0 (from ddgs)\n",
      "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting lxml>=6.0.0 (from ddgs)\n",
      "  Downloading lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.28.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\n",
      "Requirement already satisfied: brotli in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.1.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.3.0)\n",
      "Collecting socksio==1.* (from httpx[brotli,http2,socks]>=0.28.1->ddgs)\n",
      "  Downloading socksio-1.0.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.15.0)\n",
      "Downloading ddgs-9.6.1-py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading socksio-1.0.0-py3-none-any.whl (12 kB)\n",
      "Downloading lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: socksio, primp, lxml, ddgs\n",
      "  Attempting uninstall: lxml\n",
      "    Found existing installation: lxml 5.4.0\n",
      "    Uninstalling lxml-5.4.0:\n",
      "      Successfully uninstalled lxml-5.4.0\n",
      "Successfully installed ddgs-9.6.1 lxml-6.0.2 primp-0.15.0 socksio-1.0.0\n",
      "Collecting colab-xterm\n",
      "  Downloading colab_xterm-0.2.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: ptyprocess~=0.7.0 in /usr/local/lib/python3.12/dist-packages (from colab-xterm) (0.7.0)\n",
      "Requirement already satisfied: tornado>5.1 in /usr/local/lib/python3.12/dist-packages (from colab-xterm) (6.5.1)\n",
      "Downloading colab_xterm-0.2.0-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.6/115.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: colab-xterm\n",
      "Successfully installed colab-xterm-0.2.0\n",
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-1.0.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.0.0 (from langchain-ollama)\n",
      "  Downloading langchain_core-1.0.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting ollama<1.0.0,>=0.6.0 (from langchain-ollama)\n",
      "  Downloading ollama-0.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.4.38)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.11.10)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (4.15.0)\n",
      "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama<1.0.0,>=0.6.0->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.32.4)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (1.3.1)\n",
      "Downloading langchain_ollama-1.0.0-py3-none-any.whl (29 kB)\n",
      "Downloading langchain_core-1.0.2-py3-none-any.whl (469 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m469.3/469.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ollama-0.6.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: ollama, langchain-core, langchain-ollama\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.79\n",
      "    Uninstalling langchain-core-0.3.79:\n",
      "      Successfully uninstalled langchain-core-0.3.79\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-core-1.0.2 langchain-ollama-1.0.0 ollama-0.6.0\n",
      "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
      "Hit:3 https://cli.github.com/packages stable InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,396 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,479 kB]\n",
      "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,816 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,135 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,288 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,838 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
      "Fetched 28.9 MB in 2s (12.4 MB/s)\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libpci3 pci.ids\n",
      "The following NEW packages will be installed:\n",
      "  libpci3 pci.ids pciutils\n",
      "0 upgraded, 3 newly installed, 0 to remove and 48 not upgraded.\n",
      "Need to get 343 kB of archives.\n",
      "After this operation, 1,581 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 pci.ids all 0.0~2022.01.22-1ubuntu0.1 [251 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpci3 amd64 1:3.7.0-6 [28.9 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 pciutils amd64 1:3.7.0-6 [63.6 kB]\n",
      "Fetched 343 kB in 1s (473 kB/s)\n",
      "Selecting previously unselected package pci.ids.\n",
      "(Reading database ... 125079 files and directories currently installed.)\n",
      "Preparing to unpack .../pci.ids_0.0~2022.01.22-1ubuntu0.1_all.deb ...\n",
      "Unpacking pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libpci3:amd64.\n",
      "Preparing to unpack .../libpci3_1%3a3.7.0-6_amd64.deb ...\n",
      "Unpacking libpci3:amd64 (1:3.7.0-6) ...\n",
      "Selecting previously unselected package pciutils.\n",
      "Preparing to unpack .../pciutils_1%3a3.7.0-6_amd64.deb ...\n",
      "Unpacking pciutils (1:3.7.0-6) ...\n",
      "Setting up pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
      "Setting up libpci3:amd64 (1:3.7.0-6) ...\n",
      "Setting up pciutils (1:3.7.0-6) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.2)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
      "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
      "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.38)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
      "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.11.10)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (25.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.10.5)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.33.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-text-splitters, langchain-classic, langchain-community\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.4\n",
      "    Uninstalling requests-2.32.4:\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.11\n",
      "    Uninstalling langchain-text-splitters-0.3.11:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
      "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
      "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.2 which is incompatible.\n",
      "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-text-splitters-1.0.0 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb\n",
    "!pip install litellm\n",
    "!pip install qdrant-client\n",
    "!pip install -U ddgs\n",
    "!pip install colab-xterm\n",
    "!pip install -U langchain-ollama\n",
    "!apt-get update && apt-get install -y pciutils\n",
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 13614,
     "status": "ok",
     "timestamp": 1761855472727,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "dNn7j9kkaRDi",
    "outputId": "015be1c7-188c-4cd5-fa9a-322583a2f9bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-1.0.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.0.2)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (0.4.38)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (2.11.10)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-openai) (4.15.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.11.1)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.5)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-openai) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-openai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.3.0)\n",
      "Downloading langchain_openai-1.0.1-py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langchain-openai\n",
      "Successfully installed langchain-openai-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] =\n",
    "\n",
    "def Run(message: str) -> str:\n",
    "    chat = ChatOpenAI(\n",
    "        model=LLMName   # change to whichever ChatGPT model you want\n",
    "#         temperature=0          # make it deterministic\n",
    "    )\n",
    "    response = chat([HumanMessage(content=message)])\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 508,
     "status": "ok",
     "timestamp": 1761855617257,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "YTnMiKU1EozV",
    "outputId": "68c3e8cc-c6a8-4ffb-e2fc-577fbd5a8664"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import json\n",
    "from typing import Optional\n",
    "import requests\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import SystemMessage\n",
    "import warnings\n",
    "from math import ceil\n",
    "import csv\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=DeprecationWarning,\n",
    "    module=r\"jupyter_client\\.session\"\n",
    ")\n",
    "hazards = ['Flood', 'Landslide', 'Storm', 'Tsunami', 'Hurricane', 'Wildfire','Earthquake']\n",
    "drive.mount(\"/content/drive\")\n",
    "LLMName = 'gpt-5-nano'\n",
    "llm = ChatOllama(model=LLMName, num_ctx=8192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AmijBxcE_iL"
   },
   "source": [
    "### Agent Toolkits ğŸ”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15386,
     "status": "ok",
     "timestamp": 1761855637816,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "DuADPqb_KPQu"
   },
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, Any, Iterable, Union\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Initialize OpenAI client\n",
    "os.environ[\"OPENAI_API_KEY\"] =\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 97115,
     "status": "aborted",
     "timestamp": 1761855489199,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "YCfbFGMx_WsG"
   },
   "outputs": [],
   "source": [
    "# ğŸ” Tool 0: LLM Hazard Agent Router\n",
    "ROUTER_PROMPT = \"\"\"\n",
    "You are a routing expert agent. Your task is to classify a userâ€™s question into probabilities over natural hazard categories.\n",
    "These probabilities will determine which hazard expert (RAG) agent(s) are utilized (>=0.2 indicate the expert should be utilized), note to the clearly given hazard event in question (e.g. during the hurricane, the earthquake in year, or the year's storm) should be the main agent(>=0.5).\n",
    "\n",
    "Requirements:\n",
    "- Output only a valid **JSON object**.\n",
    "- Use the hazard categories as keys.\n",
    "- Values must be normalized probabilities that sum to 1.0.\n",
    "- Do not include explanations or extra text.\n",
    "\n",
    "Hazard categories:\n",
    "[\"Wildfire\", \"Storm\", \"Landslide\", \"Hurricane\", \"Flood\", \"Earthquake\",\"Tsunami\"]\n",
    "\n",
    "Question:\n",
    "{QUESTION}\n",
    "\n",
    "Output Example:\n",
    "{{\n",
    "  \"Wildfire\": 0.01,\n",
    "  \"Storm\": 0.10,\n",
    "  \"Landslide\": 0.05,\n",
    "  \"Hurricane\": 0.61,\n",
    "  \"Flood\": 0.21,\n",
    "  \"Earthquake\": 0.01,\n",
    "  \"Tsunami\":0.01\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def extract_json(text: str):\n",
    "    try:\n",
    "        # grab JSON block if extra text\n",
    "        match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "        if not match:\n",
    "            return None\n",
    "        data = json.loads(match.group())\n",
    "        return data\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def validate_probs(data: dict):\n",
    "    if not data:\n",
    "        return False\n",
    "    # all hazards present\n",
    "    if set(data.keys()) != set(hazards):\n",
    "        return False\n",
    "    # all values are floats\n",
    "    try:\n",
    "        vals = [float(data[h]) for h in hazards]\n",
    "    except Exception:\n",
    "        return False\n",
    "    # normalize check\n",
    "    return abs(sum(vals) - 1.0) < 1e-3\n",
    "\n",
    "def hazard_router(question: str, max_retries: int = 50):\n",
    "    for attempt in range(max_retries):\n",
    "        prompt = ROUTER_PROMPT.format(QUESTION=question)\n",
    "        resp = llm.invoke(prompt).content\n",
    "        data = extract_json(resp)\n",
    "        if validate_probs(data):\n",
    "            hazards = [haz for haz, prob in data.items() if prob >= 0.2]\n",
    "            return hazards\n",
    "        print(f\"Retry {attempt+1}/{max_retries} failed, retrying...\")\n",
    "    raise RuntimeError(\"Failed to extract valid hazard probabilities after retries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 97115,
     "status": "aborted",
     "timestamp": 1761855489201,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "CGOAc4rRE-6x"
   },
   "outputs": [],
   "source": [
    "# ğŸ” Tool 1: Oneline Search\n",
    "def format_search_results(results):\n",
    "    return \"\\n\\n\".join(doc[\"body\"] for doc in results)\n",
    "\n",
    "question = 'What is the SSHS category of 2017 Hurricane Harvey?'\n",
    "results = DDGS().text(question, max_results=2)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 97115,
     "status": "aborted",
     "timestamp": 1761855489202,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "RlM7A8i8Li0o"
   },
   "outputs": [],
   "source": [
    "# ğŸ” Tool 2: RAG agents\n",
    "\n",
    "# Embedding\n",
    "def embed(query: str) -> List[float]:\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-large\",\n",
    "        input=query\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Cross Encoder & Reranking\n",
    "_CROSS_ENCODER_MODEL_NAME = \"cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\"\n",
    "_cross_encoder_singleton: CrossEncoder = None\n",
    "\n",
    "def _get_cross_encoder() -> CrossEncoder:\n",
    "    global _cross_encoder_singleton\n",
    "    if _cross_encoder_singleton is None:\n",
    "        _cross_encoder_singleton = CrossEncoder(_CROSS_ENCODER_MODEL_NAME)\n",
    "    return _cross_encoder_singleton\n",
    "\n",
    "def rerank(query: str, retrieved_chunks: List[str], top_k: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Cross-encoder reranking. Returns top_k chunks sorted by relevance.\n",
    "    \"\"\"\n",
    "    if not retrieved_chunks:\n",
    "        return []\n",
    "    cross_encoder = _get_cross_encoder()\n",
    "    pairs = [(query, chunk) for chunk in retrieved_chunks]\n",
    "    scores = cross_encoder.predict(pairs)  # higher is better\n",
    "\n",
    "    # Pair each chunk with its score, sort descending, take top_k\n",
    "    scored_chunks = sorted(zip(retrieved_chunks, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [chunk for chunk, _ in scored_chunks[:top_k]]\n",
    "\n",
    "# Bi-encoder retrieval\n",
    "def _retrieve_bi(collection: chromadb.Collection, query_embedding: List[float], k: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve top-k documents from a Chroma collection using the bi-encoder embedding.\n",
    "    Returns: list[str] of document texts.\n",
    "    \"\"\"\n",
    "    res = collection.query(\n",
    "        query_embeddings=[query_embedding],  # list of one embedding\n",
    "        n_results=k\n",
    "    )\n",
    "    # Chroma returns 'documents' as List[List[str]] (one list per query)\n",
    "    docs_nested = res.get(\"documents\") or []\n",
    "    return (docs_nested[0] if docs_nested else [])  # flatten the single-query case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaO8isYJUEKk"
   },
   "source": [
    "## Agentic RAG Framework ğŸ¤–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CtzYb2BWTZ5"
   },
   "source": [
    "#### Main loop Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761855637819,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "A-ljbX-6WWFy"
   },
   "outputs": [],
   "source": [
    "def load_json_items(path: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load items from either:\n",
    "      - JSON Lines (one JSON object per line), or\n",
    "      - a single JSON array file.\n",
    "    Returns a list of dicts.\n",
    "    \"\"\"\n",
    "    txt = path.read_text(encoding=\"utf-8\").strip()\n",
    "    items: List[Dict[str, Any]] = []\n",
    "    # Try JSON array first\n",
    "    try:\n",
    "        data = json.loads(txt)\n",
    "        if isinstance(data, list):\n",
    "            items = data\n",
    "        elif isinstance(data, dict):\n",
    "            items = [data]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported JSON top-level type.\")\n",
    "        return items\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback: treat as JSONL\n",
    "    for line in txt.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        items.append(json.loads(line))\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1761855637822,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "3b4RZah8UDoc"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Model runner\n",
    "# --------------------\n",
    "# --------------------\n",
    "# Prompt templates\n",
    "# --------------------\n",
    "TF_PROMPT = (\n",
    "    \"You are an expert in hazards and resilience, evaluating the historical hazard: {disaster_name}.\\n\\n\"\n",
    "    \"====================\\n\"\n",
    "    \"STATEMENT TO JUDGE:\\n\"\n",
    "    \"{statement}\\n\"\n",
    "    \"====================\\n\\n\"\n",
    "    \"Related excerpts:\\n{joined_chunks}\\n\\n\"\n",
    "    \"Your task: Judge the statement strictly based on the excerpts of information provided by renaissance reports.\\n\"\n",
    "    \"====================\\n\"\n",
    "    \"STATEMENT TO JUDGE:\\n\"\n",
    "    \"{statement}\\n\"\n",
    "    \"====================\\n\\n\"\n",
    "    \"Answer with ONLY one word: true OR false\\n\"\n",
    "    \"No punctuation, no explanation, no extra words.\"\n",
    ")\n",
    "\n",
    "\n",
    "MC_PROMPT = (\n",
    "    \"You are an expert in hazards and resilience, evaluating the historical hazard: {disaster_name}.\\n\\n\"\n",
    "    \"====================\\n\"\n",
    "    \"QUESTION:\\n\"\n",
    "    \"{question}\\n\\n\"\n",
    "    \"OPTIONS:\\n\"\n",
    "    \"{options_block}\\n\"\n",
    "    \"====================\\n\\n\"\n",
    "    \"Related excerpts:\\n{joined_chunks}\\n\\n\"\n",
    "    \"Your task: Choose the single BEST answer to the question strictly based on the excerpts provided by renaissance reports.\\n\"\n",
    "    \"====================\\n\"\n",
    "    \"QUESTION:\\n\"\n",
    "    \"{question}\\n\\n\"\n",
    "    \"OPTIONS:\\n\"\n",
    "    \"{options_block}\\n\"\n",
    "    \"====================\\n\\n\"\n",
    "    \"Answer with ONLY one letter: A B C or D\\n\"\n",
    "    \"No punctuation, no explanation, no extra words.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761855637824,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "SsVo5egk9tkp"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Prompt templates\n",
    "# --------------------\n",
    "TF_PROMPT_no_info = (\n",
    "    \"You are an expert in hazards and resilience, evaluating the historical hazard: {disaster_name}.\\n\\n\"\n",
    "    \"====================\\n\"\n",
    "    \"STATEMENT TO JUDGE:\\n\"\n",
    "    \"{statement}\\n\"\n",
    "    \"====================\\n\\n\"\n",
    "    \"Your task: Judge the statement considering the excerpts of information.\\n\"\n",
    "    \"====================\\n\"\n",
    "    \"STATEMENT TO JUDGE:\\n\"\n",
    "    \"{statement}\\n\"\n",
    "    \"====================\\n\\n\"\n",
    "    \"Answer with ONLY one word: true OR false\\n\"\n",
    "    \"No punctuation, no explanation, no extra words.\"\n",
    ")\n",
    "\n",
    "MC_PROMPT_no_info = (\n",
    "    \"You are an expert in hazards and resilience, evaluating the historical hazard: {disaster_name}.\\n\\n\"\n",
    "    \"====================\\n\"\n",
    "    \"QUESTION:\\n\"\n",
    "    \"{question}\\n\\n\"\n",
    "    \"OPTIONS:\\n\"\n",
    "    \"{options_block}\\n\"\n",
    "    \"====================\\n\\n\"\n",
    "    \"Your task: Choose the single BEST answer to the question.\\n\"\n",
    "    \"====================\\n\"\n",
    "    \"QUESTION:\\n\"\n",
    "    \"{question}\\n\\n\"\n",
    "    \"OPTIONS:\\n\"\n",
    "    \"{options_block}\\n\"\n",
    "    \"====================\\n\\n\"\n",
    "    \"Answer with ONLY one letter: A B C or D\\n\"\n",
    "    \"No punctuation, no explanation, no extra words.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1761855637832,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "st2qsOukWg3I"
   },
   "outputs": [],
   "source": [
    "def normalize_tf(ans: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize model output to 'true' or 'false' strictly.\n",
    "    Falls back to regex keyword detection.\n",
    "    \"\"\"\n",
    "    s = ans.strip().lower()\n",
    "    if s in {\"true\", \"false\"}:\n",
    "        return s\n",
    "    # Heuristics\n",
    "    if \"true\" in s and \"false\" not in s:\n",
    "        return \"true\"\n",
    "    if \"false\" in s and \"true\" not in s:\n",
    "        return \"false\"\n",
    "    # If ambiguous, keep first token\n",
    "    return \"true\" if s.startswith(\"t\") else \"false\"\n",
    "\n",
    "def normalize_mc(ans: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize model output to a single uppercase letter A-D.\n",
    "    \"\"\"\n",
    "    s = ans.strip().upper()\n",
    "    # Grab first A/B/C/D we can find\n",
    "    m = re.search(r\"\\b([ABCD])\\b\", s)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    # Fallback: first character if valid\n",
    "    if s[:1] in {\"A\", \"B\", \"C\", \"D\"}:\n",
    "        return s[:1]\n",
    "    # Last resort: A\n",
    "    return \"A\"\n",
    "\n",
    "def options_to_block(options: Iterable[str]) -> str:\n",
    "    \"\"\"\n",
    "    Ensure options appear one per line as 'A. ...' etc.\n",
    "    If options are already labeled, keep them; else auto-label.\n",
    "    \"\"\"\n",
    "    opts = list(options)\n",
    "    labeled = all(re.match(r\"^[ABCD]\\s*\\.\", o.strip()) for o in opts)\n",
    "    if labeled:\n",
    "        return \"\\n\".join(o if o.endswith(\"\\n\") else o for o in opts)\n",
    "    labels = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    return \"\\n\".join(f\"{labels[i]}. {opts[i]}\" for i in range(min(4, len(opts))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVBapGQWWYDh"
   },
   "source": [
    "#### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1761857363697,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "r-e5BLMczcdI",
    "outputId": "3f75a066-b9f3-48f8-c853-ced4bb650d05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Flood ===\n",
      "Total Valid QA: 658\n",
      "\n",
      "=== Landslide ===\n",
      "Total Valid QA: 263\n",
      "\n",
      "=== Storm ===\n",
      "Total Valid QA: 348\n",
      "\n",
      "=== Tsunami ===\n",
      "Total Valid QA: 70\n",
      "\n",
      "=== Hurricane ===\n",
      "Total Valid QA: 891\n",
      "\n",
      "=== Wildfire ===\n",
      "Total Valid QA: 92\n",
      "\n",
      "=== Earthquake ===\n",
      "Total Valid QA: 3454\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "def load_json_items(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    # handle both list and dict-based formats\n",
    "    if isinstance(data, dict) and \"data\" in data:\n",
    "        return data[\"data\"]\n",
    "    return data\n",
    "\n",
    "categories = ['Flood', 'Landslide', 'Storm', 'Tsunami', 'Hurricane', 'Wildfire', 'Earthquake']\n",
    "QA_folder = Path(\"/content/drive/MyDrive/LLMs/LLM_Hazards/New_QA_Cate\")\n",
    "\n",
    "for category in categories:\n",
    "    QA_path = QA_folder / category\n",
    "\n",
    "    total_count = 0\n",
    "    qtype_counter = Counter()\n",
    "    cate1_counter = Counter()\n",
    "    combo_counter = Counter()   # category1 Ã— qtype joint count\n",
    "\n",
    "    for json_path in sorted(QA_path.glob(\"*.json\")):\n",
    "        items = load_json_items(json_path)\n",
    "        for obj in items:\n",
    "            if obj is None:\n",
    "                continue\n",
    "\n",
    "            qtype = obj.get(\"question_type\", \"\").strip().lower()\n",
    "            category1 = obj.get(\"category\", \"\").strip()\n",
    "            if category1 == \"Invalid\":\n",
    "                continue\n",
    "\n",
    "            # Count overall and per type\n",
    "            if qtype in [\"true_false\", \"mc_single\"]:\n",
    "                total_count += 1\n",
    "                qtype_counter[qtype] += 1\n",
    "                cate1_counter[category1] += 1\n",
    "                combo_counter[(category1, qtype)] += 1\n",
    "\n",
    "    print(f\"\\n=== {category} ===\")\n",
    "    print(f\"Total Valid QA: {total_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aISrfMw10H0F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1761855679292,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "tBf8YnCXxA4-",
    "outputId": "0ff1f81b-a77e-4af0-8a44-c57e847f79d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flood Count: 658\n",
      "Landslide Count: 263\n",
      "Storm Count: 348\n",
      "Tsunami Count: 70\n",
      "Hurricane Count: 891\n",
      "Wildfire Count: 92\n",
      "Earthquake Count: 3454\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# Main loop\n",
    "# --------------------\n",
    "categories = ['Flood', 'Landslide', 'Storm', 'Tsunami', 'Hurricane', 'Wildfire','Earthquake']\n",
    "QA_folder = Path(\"/content/drive/MyDrive/LLMs/LLM_Hazards/New_QA_Cate\")\n",
    "\n",
    "for category in categories:\n",
    "    QA_path = QA_folder / category\n",
    "    # Iterate all .json files\n",
    "    # Counter\n",
    "    count = 0\n",
    "    for json_path in sorted(QA_path.glob(\"*.json\")):\n",
    "        items = load_json_items(json_path)\n",
    "        rows = []\n",
    "        for obj in items:\n",
    "            if obj is None:\n",
    "              continue\n",
    "            qtype = obj.get(\"question_type\", \"\").strip().lower()\n",
    "            qa = obj.get(\"question_answer\", {})\n",
    "            disaster_type = obj.get(\"disaster_type\", \"\")\n",
    "            category1 = obj.get(\"category\", \"\")\n",
    "            disaster_name = obj.get(\"disaster_name\", \"\")\n",
    "\n",
    "            if category1 == \"Invalid\":\n",
    "                continue\n",
    "\n",
    "            if qtype == \"true_false\":\n",
    "                count = count +1\n",
    "\n",
    "            elif qtype == \"mc_single\":\n",
    "                if category1 == \"Invalid\":\n",
    "                    continue\n",
    "                count = count +1\n",
    "    print(f\"{category} Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 97119,
     "status": "aborted",
     "timestamp": 1761855489209,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "IEHtyDMoVNEo"
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Main loop\n",
    "# --------------------\n",
    "categories = ['Flood', 'Landslide', 'Storm', 'Tsunami', 'Hurricane', 'Wildfire','Earthquake']\n",
    "QA_folder = Path(\"/content/drive/MyDrive/LLMs/LLM_Hazards/New_QA_Cate\")\n",
    "\n",
    "for category in categories:\n",
    "    QA_path = QA_folder / category\n",
    "    # Iterate all .json files\n",
    "    # Counter\n",
    "    n=0\n",
    "    for json_path in sorted(QA_path.glob(\"*.json\")):\n",
    "        n=n+1\n",
    "        if n>4:\n",
    "          continue\n",
    "        items = load_json_items(json_path)\n",
    "        rows = []\n",
    "        for obj in items:\n",
    "            if obj is None:\n",
    "              continue\n",
    "            qtype = obj.get(\"question_type\", \"\").strip().lower()\n",
    "            qa = obj.get(\"question_answer\", {})\n",
    "            disaster_type = obj.get(\"disaster_type\", \"\")\n",
    "            category1 = obj.get(\"category\", \"\")\n",
    "            disaster_name = obj.get(\"disaster_name\", \"\")\n",
    "\n",
    "            if category1 == \"Invalid\":\n",
    "                print(f\"Skipping invalid category: {category1}\")\n",
    "                continue\n",
    "\n",
    "            if qtype == \"true_false\":\n",
    "                statement = qa.get(\"statement\", \"\")\n",
    "                gt = qa.get(\"answer\", None)  # True/False (bool) if provided\n",
    "                rag_query = f\"{disaster_name} {statement}\"\n",
    "\n",
    "                # Router + RAG Agents\n",
    "                joined_chunks = []\n",
    "                hazard_list = []\n",
    "\n",
    "                # Define values\n",
    "                is_enough_online = '-1'\n",
    "                is_enough_rag = '-1'\n",
    "\n",
    "                message = TF_PROMPT_no_info.format(disaster_name = disaster_name,statement=statement)\n",
    "\n",
    "                raw = Run(message)\n",
    "                pred = normalize_tf(raw)\n",
    "                print(pred,str(gt))\n",
    "\n",
    "                is_correct = None\n",
    "                if isinstance(gt, bool):\n",
    "                    is_correct = (pred == str(gt).lower())\n",
    "\n",
    "                rows.append({\n",
    "                    \"disaster_type\": disaster_type,\n",
    "                    \"disaster_name\": disaster_name,\n",
    "                    \"category\": category,\n",
    "                    \"category1\": category1,\n",
    "                    \"file\": json_path.name,\n",
    "                    \"question_type\": \"true_false\",\n",
    "                    \"question_or_statement\": statement,\n",
    "                    \"options\": \"\",\n",
    "                    \"router\":str(hazard_list),\n",
    "                    \"retreived_chunks\": joined_chunks,\n",
    "                    \"ground_truth\": str(gt).lower() if isinstance(gt, bool) else \"\",\n",
    "                    \"model_raw\": raw,\n",
    "                    \"model_pred\": pred,\n",
    "                    \"is_correct\": is_correct,\n",
    "                    \"is_enough_rag\": is_enough_rag,\n",
    "                    \"is_enough_online\": is_enough_online\n",
    "                })\n",
    "\n",
    "            elif qtype == \"mc_single\":\n",
    "                question = qa.get(\"question\", \"\")\n",
    "                options = qa.get(\"options\", [])\n",
    "                correct = qa.get(\"correct\", \"\")  # e.g., \"A\"\n",
    "                options_block = options_to_block(options)\n",
    "                category1 = obj.get(\"category\", \"\")\n",
    "                if category1 == \"Invalid\":\n",
    "                    print(f\"Skipping invalid category: {category1}\")\n",
    "                    continue\n",
    "                # Retrieval and Rerank here\n",
    "                rag_query = f\"{disaster_name} {question}\"\n",
    "\n",
    "                # Router + RAG Agents\n",
    "                joined_chunks = []\n",
    "                hazard_list = []\n",
    "\n",
    "                # Define values\n",
    "                is_enough_online = '-1'\n",
    "                is_enough_rag = '-1'\n",
    "\n",
    "\n",
    "                message = MC_PROMPT_no_info.format(disaster_name = disaster_name,question=question,options_block = options_block)\n",
    "\n",
    "                # print(message)\n",
    "\n",
    "                raw = Run(message)\n",
    "                pred = normalize_mc(raw)\n",
    "                print(pred,correct)\n",
    "\n",
    "                is_correct = (pred == correct.strip().upper()) if correct else None\n",
    "\n",
    "                rows.append({\n",
    "                    \"disaster_type\": disaster_type,\n",
    "                    \"disaster_name\": disaster_name,\n",
    "                    \"category\": category,\n",
    "                    \"category1\": category1,\n",
    "                    \"file\": json_path.name,\n",
    "                    \"question_type\": \"mc_single\",\n",
    "                    \"question_or_statement\": question,\n",
    "                    \"options\": \" | \".join(options),\n",
    "                    \"router\":str(hazard_list),\n",
    "                    \"retreived_chunks\": joined_chunks,\n",
    "                    \"ground_truth\": correct.strip().upper() if correct else \"\",\n",
    "                    \"model_raw\": raw,\n",
    "                    \"model_pred\": pred,\n",
    "                    \"is_correct\": is_correct,\n",
    "                    \"is_enough_rag\": is_enough_rag,\n",
    "                    \"is_enough_online\": is_enough_online\n",
    "                })\n",
    "\n",
    "            else:\n",
    "                # Unknown type: skip gracefully\n",
    "                rows.append({\n",
    "                    \"disaster_type\": disaster_type,\n",
    "                    \"disaster_name\": disaster_name,\n",
    "                    \"category\": category,\n",
    "                    \"category1\": category1,\n",
    "                    \"file\": json_path.name,\n",
    "                    \"question_type\": qtype,\n",
    "                    \"question_or_statement\": \"\",\n",
    "                    \"options\": \"\",\n",
    "                    \"router\":str(hazard_list),\n",
    "                    \"retreived_chunks\": joined_chunks,\n",
    "                    \"ground_truth\": \"\",\n",
    "                    \"model_raw\": \"\",\n",
    "                    \"model_pred\": \"\",\n",
    "                    \"is_correct\": None,\n",
    "                    \"is_enough_rag\": None,\n",
    "                    \"is_enough_online\": None\n",
    "                })\n",
    "\n",
    "        # Save answers to CSV in the corresponding model/category output folder\n",
    "        output_folder = Path(\"/content/drive/MyDrive/LLMs/LLM_Hazards/Final_Experiments/Outputs/Output_zero_shot_llama\")\n",
    "        out_dir = output_folder / category\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        out_csv = out_dir / (json_path.stem + \".csv\")\n",
    "\n",
    "        fieldnames = [\n",
    "            \"disaster_type\", \"disaster_name\", \"category\",\"category1\", \"file\",\n",
    "            \"question_type\", \"question_or_statement\", \"options\",\"router\",\"retreived_chunks\",\n",
    "            \"ground_truth\", \"model_raw\", \"model_pred\", \"is_correct\",\"is_enough_rag\",\"is_enough_online\"\n",
    "        ]\n",
    "\n",
    "        with out_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for r in rows:\n",
    "                writer.writerow(r)\n",
    "\n",
    "        print(f\"Saved: {out_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liZHZvkjzf-f"
   },
   "source": [
    "### Result performance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 97120,
     "status": "aborted",
     "timestamp": 1761855489210,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "7MUI5u1c_m4A"
   },
   "outputs": [],
   "source": [
    "! pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 97121,
     "status": "aborted",
     "timestamp": 1761855489211,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "t4Y70LfXpUst"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set the root output folder where per-file CSVs were written\n",
    "root = output_folder\n",
    "\n",
    "def _to_bool(s):\n",
    "    return str(s).strip().lower() in (\"true\", \"1\", \"yes\")\n",
    "\n",
    "def _safe_float(num, den):\n",
    "    return (num / den) if den else 0.0\n",
    "\n",
    "# Collect all rows from all CSVs under the (optional) category subfolder you just used.\n",
    "# If you want to aggregate **only** one category, use: for p in (root / category).glob(\"*.csv\")\n",
    "all_rows = []\n",
    "for p in root.rglob(\"*.csv\"):\n",
    "    # skip any *_summary.csv if you created those earlier\n",
    "    if p.name.endswith(\"_summary.csv\"):\n",
    "        continue\n",
    "    with p.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for r in reader:\n",
    "            all_rows.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 97121,
     "status": "aborted",
     "timestamp": 1761855489212,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "yf8nPy65zdSn"
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate  # pip install tabulate if not installed\n",
    "\n",
    "def _to_bool(s):\n",
    "    return str(s).strip().lower() in (\"true\", \"1\", \"yes\")\n",
    "\n",
    "n = len(rows) or 1\n",
    "acc = sum(1 for r in rows if r.get(\"is_correct\") is True) / n\n",
    "rag_rate = sum(1 for r in rows if r.get(\"is_enough_rag\") == \"1\") / n\n",
    "online_rate = sum(1 for r in rows if r.get(\"is_enough_online\") == \"1\") / n\n",
    "both_insuff_rate = sum(\n",
    "    1 for r in rows\n",
    "    if r.get(\"is_enough_rag\") != \"1\" and r.get(\"is_enough_online\") == \"0\"\n",
    ") / n\n",
    "\n",
    "# Confusion matrix\n",
    "eval_rows = [r for r in rows if str(r.get(\"ground_truth\",\"\")).strip() != \"\"]\n",
    "tp = fp = fn = tn = 0\n",
    "for r in eval_rows:\n",
    "    gt = _to_bool(r.get(\"ground_truth\", \"\"))\n",
    "    pred = _to_bool(r.get(\"model_pred\", \"\"))\n",
    "    if pred and gt: tp += 1\n",
    "    elif pred and not gt: fp += 1\n",
    "    elif not pred and gt: fn += 1\n",
    "    else: tn += 1\n",
    "\n",
    "# ---- Print summary ----\n",
    "print(\"\\n=== Summary Metrics ===\")\n",
    "print(tabulate([\n",
    "    [\"Total samples\", n],\n",
    "    [\"Accuracy\", f\"{acc:.3f}\"],\n",
    "    [\"RAG sufficiency rate\", f\"{rag_rate:.3f}\"],\n",
    "    [\"Online sufficiency rate\", f\"{online_rate:.3f}\"],\n",
    "    [\"Both insufficient rate\", f\"{both_insuff_rate:.3f}\"],\n",
    "], headers=[\"Metric\", \"Value\"], tablefmt=\"github\"))\n",
    "\n",
    "print(\"\\n=== Confusion Matrix (GT vs Pred) ===\")\n",
    "print(tabulate([\n",
    "    [\"TP\", tp, \"GT=True, Pred=True\"],\n",
    "    [\"FP\", fp, \"GT=False, Pred=True\"],\n",
    "    [\"FN\", fn, \"GT=True, Pred=False\"],\n",
    "    [\"TN\", tn, \"GT=False, Pred=False\"],\n",
    "], headers=[\"Cell\", \"Count\", \"Meaning\"], tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 97122,
     "status": "aborted",
     "timestamp": 1761855489213,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "vq0SVxLNzcxZ"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set the root output folder where per-file CSVs were written\n",
    "root = output_folder\n",
    "\n",
    "def _to_bool(s):\n",
    "    return str(s).strip().lower() in (\"true\", \"1\", \"yes\")\n",
    "\n",
    "def _safe_float(num, den):\n",
    "    return (num / den) if den else 0.0\n",
    "\n",
    "# Collect all rows from all CSVs under the (optional) category subfolder you just used.\n",
    "# If you want to aggregate **only** one category, use: for p in (root / category).glob(\"*.csv\")\n",
    "all_rows = []\n",
    "for p in root.rglob(\"*.csv\"):\n",
    "    # skip any *_summary.csv if you created those earlier\n",
    "    if p.name.endswith(\"_summary.csv\"):\n",
    "        continue\n",
    "    with p.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for r in reader:\n",
    "            all_rows.append(r)\n",
    "\n",
    "# --- Overall metrics ---\n",
    "n = len(all_rows)\n",
    "correct = sum(1 for r in all_rows if r.get(\"is_correct\") in (True, \"True\", \"true\", \"1\"))\n",
    "acc = _safe_float(correct, n)\n",
    "\n",
    "rag_sufficient = sum(1 for r in all_rows if r.get(\"is_enough_rag\") == \"1\")\n",
    "online_sufficient = sum(1 for r in all_rows if r.get(\"is_enough_online\") == \"1\")\n",
    "both_insufficient = sum(1 for r in all_rows if r.get(\"is_enough_rag\") != \"1\" and r.get(\"is_enough_online\") == \"0\")\n",
    "\n",
    "rag_rate = _safe_float(rag_sufficient, n)\n",
    "online_rate = _safe_float(online_sufficient, n)\n",
    "both_insuff_rate = _safe_float(both_insufficient, n)\n",
    "\n",
    "# --- Confusion matrix (only rows that have ground_truth) ---\n",
    "eval_rows = [r for r in all_rows if str(r.get(\"ground_truth\", \"\")).strip() != \"\"]\n",
    "tp = fp = fn = tn = 0\n",
    "for r in eval_rows:\n",
    "    gt = _to_bool(r.get(\"ground_truth\", \"\"))\n",
    "    pred = _to_bool(r.get(\"model_pred\", \"\"))\n",
    "    if pred and gt:\n",
    "        tp += 1\n",
    "    elif pred and not gt:\n",
    "        fp += 1\n",
    "    elif not pred and gt:\n",
    "        fn += 1\n",
    "    else:\n",
    "        tn += 1\n",
    "\n",
    "# --- Per-category accuracy ---\n",
    "by_cat = defaultdict(lambda: {\"n\": 0, \"correct\": 0})\n",
    "for r in all_rows:\n",
    "    cat = r.get(\"category\", \"UNKNOWN\")\n",
    "    by_cat[cat][\"n\"] += 1\n",
    "    if r.get(\"is_correct\") in (True, \"True\", \"true\", \"1\"):\n",
    "        by_cat[cat][\"correct\"] += 1\n",
    "\n",
    "# ---- Print summary ----\n",
    "print(\"\\n=== Overall Summary ===\")\n",
    "print(f\"Total samples: {n}\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"RAG sufficiency rate: {rag_rate:.4f}\")\n",
    "print(f\"Online sufficiency rate: {online_rate:.4f}\")\n",
    "print(f\"Both insufficient rate: {both_insuff_rate:.4f}\")\n",
    "\n",
    "print(\"\\n=== Confusion Matrix (GT vs Pred) ===\")\n",
    "print(f\"TP: {tp} | FP: {fp} | FN: {fn} | TN: {tn}\")\n",
    "total_eval = tp + fp + fn + tn\n",
    "if total_eval:\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0.0\n",
    "    print(f\"Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "print(\"\\n=== Per-Category Accuracy ===\")\n",
    "for cat, d in sorted(by_cat.items(), key=lambda x: x[0]):\n",
    "    cat_acc = _safe_float(d[\"correct\"], d[\"n\"])\n",
    "    print(f\"- {cat}: {cat_acc:.4f} ({d['correct']}/{d['n']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 97130,
     "status": "aborted",
     "timestamp": 1761855489221,
     "user": {
      "displayName": "Chenchen Kuai",
      "userId": "16426867159365045469"
     },
     "user_tz": 300
    },
    "id": "p6qGtPOvJqyD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM7uKPufBwZIeYZhqDi1Ysj",
   "collapsed_sections": [
    "8AmijBxcE_iL",
    "liZHZvkjzf-f"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
